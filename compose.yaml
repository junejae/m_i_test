name: mig-inference

x-vllm-common: &vllm-common
  image: vllm/vllm-openai:latest
  ipc: host
  runtime: nvidia
  environment:
    NVIDIA_DRIVER_CAPABILITIES: compute,utility
    HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
  volumes:
    - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
  restart: unless-stopped
  healthcheck:
    test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:8000/health', timeout=3)"]
    interval: 20s
    timeout: 5s
    retries: 15

services:
  # GPU 0 is already in use elsewhere and is intentionally not touched.
  # Every service below is pinned to a MIG instance created on GPU 1.
  mig-vllm-1:
    <<: *vllm-common
    container_name: mig-vllm-1
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_1}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    command: >
      ${MODEL_1}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${SERVED_MODEL_NAME_1}
      --dtype ${DTYPE_1}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_1}
      --max-model-len ${MAX_MODEL_LEN_1}
      --max-num-seqs ${MAX_NUM_SEQS_1}
      --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS_1}
      --limit-mm-per-prompt.image 0
      --limit-mm-per-prompt.video 0
      --tensor-parallel-size 1
      ${VLLM_EXTRA_ARGS_1}
    ports:
      - "${PORT_1}:8000"

  mig-vllm-2:
    <<: *vllm-common
    container_name: mig-vllm-2
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_2}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    command: >
      ${MODEL_2}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${SERVED_MODEL_NAME_2}
      --dtype ${DTYPE_2}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_2}
      --max-model-len ${MAX_MODEL_LEN_2}
      --max-num-seqs ${MAX_NUM_SEQS_2}
      --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS_2}
      --limit-mm-per-prompt.image ${MM_IMAGE_LIMIT_2}
      --limit-mm-per-prompt.video ${MM_VIDEO_LIMIT_2}
      --tensor-parallel-size 1
      ${VLLM_EXTRA_ARGS_2}
    ports:
      - "${PORT_2}:8000"

  mig-vllm-3:
    <<: *vllm-common
    container_name: mig-vllm-3
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_3}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    command: >
      ${MODEL_3}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${SERVED_MODEL_NAME_3}
      --dtype ${DTYPE_3}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_3}
      --max-model-len ${MAX_MODEL_LEN_3}
      --max-num-seqs ${MAX_NUM_SEQS_3}
      --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS_3}
      --tensor-parallel-size 1
      ${VLLM_EXTRA_ARGS_3}
    ports:
      - "${PORT_3}:8000"

  mig-vllm-4:
    <<: *vllm-common
    container_name: mig-vllm-4
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_4}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    command: >
      ${MODEL_4}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${SERVED_MODEL_NAME_4}
      --dtype ${DTYPE_4}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_4}
      --max-model-len ${MAX_MODEL_LEN_4}
      --max-num-seqs ${MAX_NUM_SEQS_4}
      --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS_4}
      --tensor-parallel-size 1
      ${VLLM_EXTRA_ARGS_4}
    ports:
      - "${PORT_4}:8000"

  mig-vllm-5:
    <<: *vllm-common
    container_name: mig-vllm-5
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_5}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    command: >
      ${MODEL_5}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${SERVED_MODEL_NAME_5}
      --task ${TASK_5}
      --dtype ${DTYPE_5}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_5}
      --max-model-len ${MAX_MODEL_LEN_5}
      --max-num-seqs ${MAX_NUM_SEQS_5}
      --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS_5}
      --tensor-parallel-size 1
      ${VLLM_EXTRA_ARGS_5}
    ports:
      - "${PORT_5}:8000"
