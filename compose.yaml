name: mig-inference

x-vllm-common: &vllm-common
  image: vllm/vllm-openai:latest
  ipc: host
  runtime: nvidia
  environment:
    NVIDIA_DRIVER_CAPABILITIES: compute,utility
    HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
  volumes:
    - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
  restart: unless-stopped
  healthcheck:
    test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:8000/health', timeout=3)"]
    interval: 20s
    timeout: 5s
    retries: 15

services:
  # GPU 0 is already in use elsewhere and is intentionally not touched.
  # Every service below is pinned to a MIG instance created on GPU 1.
  mig-vllm-1:
    <<: *vllm-common
    container_name: mig-vllm-1
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_1}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    command: >
      ${MODEL_1}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${SERVED_MODEL_NAME_1}
      --dtype ${DTYPE_1}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_1}
      --max-model-len ${MAX_MODEL_LEN_1}
      --max-num-seqs ${MAX_NUM_SEQS_1}
      --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS_1}
      --limit-mm-per-prompt.image 0
      --limit-mm-per-prompt.video 0
      --tensor-parallel-size 1
      ${VLLM_EXTRA_ARGS_1}
    ports:
      - "${PORT_1}:8000"

  mig-vllm-2:
    <<: *vllm-common
    container_name: mig-vllm-2
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_2}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    command: >
      ${MODEL_2}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${SERVED_MODEL_NAME_2}
      --dtype ${DTYPE_2}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_2}
      --max-model-len ${MAX_MODEL_LEN_2}
      --max-num-seqs ${MAX_NUM_SEQS_2}
      --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS_2}
      --limit-mm-per-prompt.image ${MM_IMAGE_LIMIT_2}
      --limit-mm-per-prompt.video ${MM_VIDEO_LIMIT_2}
      --tensor-parallel-size 1
      ${VLLM_EXTRA_ARGS_2}
    ports:
      - "${PORT_2}:8000"

  mig-vllm-3:
    <<: *vllm-common
    container_name: mig-vllm-3
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_3}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    command: >
      ${MODEL_3}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${SERVED_MODEL_NAME_3}
      --dtype ${DTYPE_3}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_3}
      --max-model-len ${MAX_MODEL_LEN_3}
      --max-num-seqs ${MAX_NUM_SEQS_3}
      --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS_3}
      --tensor-parallel-size 1
      ${VLLM_EXTRA_ARGS_3}
    ports:
      - "${PORT_3}:8000"

  mig-vllm-4:
    <<: *vllm-common
    container_name: mig-vllm-4
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_4}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    command: >
      ${MODEL_4}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${SERVED_MODEL_NAME_4}
      --dtype ${DTYPE_4}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_4}
      --max-model-len ${MAX_MODEL_LEN_4}
      --max-num-seqs ${MAX_NUM_SEQS_4}
      --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS_4}
      --tensor-parallel-size 1
      ${VLLM_EXTRA_ARGS_4}
    ports:
      - "${PORT_4}:8000"

  mig-asr-5:
    build:
      context: ./asr-server
    container_name: mig-asr-5
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_5}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
      ASR_MODEL_ID: ${ASR_MODEL_5}
      ASR_COMPUTE_TYPE: ${ASR_COMPUTE_TYPE_5}
      ASR_BEAM_SIZE: ${ASR_BEAM_SIZE_5}
      ASR_LANGUAGE: ${ASR_LANGUAGE_5}
      ASR_DEVICE: ${ASR_DEVICE_5}
    volumes:
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:8000/health', timeout=3)"]
      interval: 20s
      timeout: 5s
      retries: 15
    ports:
      - "${PORT_5}:8000"

  mig-vllm-6:
    # Qwen3-TTS requires the vLLM-Omni serving path.
    image: vllm/vllm-omni:v0.14.0
    container_name: mig-vllm-6
    ipc: host
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: ${MIG_UUID_6}
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
    volumes:
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:8000/health', timeout=3)"]
      interval: 20s
      timeout: 5s
      retries: 15
    command: >
      ${MODEL_6}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${SERVED_MODEL_NAME_6}
      --stage-configs-path vllm_omni/model_executor/stage_configs/qwen3_tts.yaml
      --omni
      --trust-remote-code
      --enforce-eager
      --dtype ${DTYPE_6}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_6}
      --max-model-len ${MAX_MODEL_LEN_6}
      --max-num-seqs ${MAX_NUM_SEQS_6}
      --max-num-batched-tokens ${MAX_NUM_BATCHED_TOKENS_6}
      --tensor-parallel-size 1
      ${VLLM_EXTRA_ARGS_6}
    ports:
      - "${PORT_6}:8000"
